import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.utils import to_categorical
import pickle

# ==========================
# å‚æ•°é…ç½®
# ==========================
n_mfcc = 40  # ä¸preprocess.pyä¸­ä¿æŒä¸€è‡´

# ==========================
# åŠ è½½å¤„ç†åçš„ç‰¹å¾å’Œæ ‡ç­¾
# ==========================
X = np.load('processed_data/X_pad.npy')  # shape: (æ ·æœ¬æ•°, æ—¶é—´æ­¥, ç‰¹å¾æ•°)
y = np.load('processed_data/y_categorical.npy')  # shape: (æ ·æœ¬æ•°, ç±»åˆ«æ•°)

# åŠ è½½æ ‡ç­¾ç¼–ç å™¨
with open('processed_data/label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

print("X shape:", X.shape)
print("y shape:", y.shape)

# æ·»åŠ é€šé“ç»´åº¦ï¼Œå˜ä¸º (æ ·æœ¬æ•°, æ—¶é—´æ­¥, ç‰¹å¾æ•°, 1)
X = np.expand_dims(X, axis=-1)

# ==========================
# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
# ==========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nè®­ç»ƒé›†æ ·æœ¬æ•°: {X_train.shape[0]}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {X_test.shape[0]}")

# ==========================
# æ„å»º CNN æ¨¡å‹
# ==========================
model = Sequential()

model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', 
                input_shape=(X.shape[1], n_mfcc, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(GlobalAveragePooling2D())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(y.shape[1], activation='softmax'))

# ==========================
# ç¼–è¯‘å¹¶è®­ç»ƒæ¨¡å‹
# ==========================
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("\nğŸš€ æ­£åœ¨è®­ç»ƒæ¨¡å‹ ...")
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=64,
    validation_data=(X_test, y_test),
    verbose=1
)

# ==========================
# æ¨¡å‹è¯„ä¼°
# ==========================
print("\nğŸ” æ¨¡å‹è¯„ä¼°ç»“æœ:")

# 1. ä½¿ç”¨model.evaluateè¯„ä¼°æµ‹è¯•é›†
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{test_acc:.4f}")
print(f"âœ… æµ‹è¯•é›†æŸå¤±ï¼š{test_loss:.4f}")

# 2. é¢„æµ‹æµ‹è¯•é›†ç»“æœ
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# 3. åˆ†ç±»æŠ¥å‘Š
print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_true_classes, y_pred_classes, 
                          target_names=label_encoder.classes_))

# 4. æ··æ·†çŸ©é˜µ
conf_mat = confusion_matrix(y_true_classes, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_mat, annot=True, fmt='d', 
            xticklabels=label_encoder.classes_, 
            yticklabels=label_encoder.classes_,
            cmap='Blues')
plt.title('æ··æ·†çŸ©é˜µ')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')
plt.savefig('confusion_matrix.png')
plt.close()
print("ğŸ“ˆ æ··æ·†çŸ©é˜µå·²ä¿å­˜ä¸º confusion_matrix.png")

# 5. å„ç±»åˆ«é¢„æµ‹æ•°é‡ç»Ÿè®¡å›¾
plt.figure(figsize=(10, 6))
unique, counts = np.unique(y_pred_classes, return_counts=True)
plt.bar(label_encoder.classes_[unique], counts)
plt.title('å„ç±»åˆ«é¢„æµ‹æ•°é‡ç»Ÿè®¡')
plt.xlabel('æƒ…æ„Ÿç±»åˆ«')
plt.ylabel('é¢„æµ‹æ•°é‡')
plt.savefig('prediction_distribution.png')
plt.close()
print("ğŸ“Š å„ç±»åˆ«é¢„æµ‹æ•°é‡ç»Ÿè®¡å›¾å·²ä¿å­˜ä¸º prediction_distribution.png")

# 6. è®­ç»ƒè¿‡ç¨‹æ›²çº¿
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
plt.title('å‡†ç¡®ç‡æ›²çº¿')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
plt.title('æŸå¤±æ›²çº¿')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig('training_history.png')
plt.close()
print("ğŸ“‰ è®­ç»ƒè¿‡ç¨‹æ›²çº¿å·²ä¿å­˜ä¸º training_history.png")

# ==========================
# ä¿å­˜æ¨¡å‹
# ==========================
model.save('cnn_model_split.keras') 
print("\nğŸ“¦ æ¨¡å‹å·²ä¿å­˜ä¸º cnn_model_split.keras")